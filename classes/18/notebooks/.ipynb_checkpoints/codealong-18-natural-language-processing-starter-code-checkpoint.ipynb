{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS-SF-34 | 18 | Natural Language Processing | Codelong | Starter Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >>> One-time setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "nltk.download()\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <<< One-time setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A | Tokenization and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "\n",
    "import string\n",
    "import unicodedata\n",
    "from nltk import tokenize, corpus, stem\n",
    "\n",
    "from sklearn import feature_extraction, linear_model, ensemble, model_selection, metrics, decomposition\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_text(document):\n",
    "    document = document.encode('utf-8')\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    document = document.lower()\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = tokenize.word_tokenize(document)\n",
    "\n",
    "    # Remove punctuation in tokens and then remove empty tokens\n",
    "    tokens = [token.translate(None, string.punctuation) for token in tokens]\n",
    "    tokens = [token for token in tokens if token]\n",
    "\n",
    "    # Remove stop words\n",
    "    tokens = [token for token in tokens if not token in corpus.stopwords.words('english')]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentence', 'wait', 'another', 'third']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenize_text(\"This is a sentence...  Wait, here's another.  And a third!\")\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Stemmer:\n",
    "    stemmer = stem.porter.PorterStemmer()\n",
    "\n",
    "    @staticmethod\n",
    "    def stem_tokens(tokens):\n",
    "        return [Stemmer.stemmer.stem(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'sentenc', 'wait', u'anoth', 'third']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = Stemmer.stem_tokens(tokens)\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B | Book reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we will be analyzing a partial list of the reviews for J.K. Rowling's The Casual Vacancy.  (https://www.amazon.com/dp/0316228532)  We scrapped this dataset during class 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join('..', 'datasets', 'dataset-18-reviews.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>star_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-04-21</td>\n",
       "      <td>R3TUANQ2EB3ECB</td>\n",
       "      <td>MichaelMichaels</td>\n",
       "      <td>Skip it. Life is too short.</td>\n",
       "      <td>I've never read any of the Harry Potter books ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-04-20</td>\n",
       "      <td>R2DD03ZZ4218VW</td>\n",
       "      <td>Frans van Wyk</td>\n",
       "      <td>Four Stars</td>\n",
       "      <td>Excellent Read with a lot of real life values ...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-04-20</td>\n",
       "      <td>R296NVKLH5QS4W</td>\n",
       "      <td>Sabina Duke</td>\n",
       "      <td>Characters</td>\n",
       "      <td>Hard to keep the characters straight</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-04-05</td>\n",
       "      <td>R3MP7W8LH6VHU8</td>\n",
       "      <td>Jen Blau</td>\n",
       "      <td>GIVE IT A CHANCE!</td>\n",
       "      <td>I almost put this book down. I'm new to Rowlin...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-04-04</td>\n",
       "      <td>RZWP48RKJCXT1</td>\n",
       "      <td>Lilith Eleanor</td>\n",
       "      <td>Frighteningly good</td>\n",
       "      <td>Amazing. Rowling combines fantastic writing wi...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5856</th>\n",
       "      <td>2012-09-27</td>\n",
       "      <td>RT2TE0W92SL67</td>\n",
       "      <td>Tricia K.</td>\n",
       "      <td>Seriously?  $17 bucks for a computer file???  ...</td>\n",
       "      <td>Premise sounds dull as dirt.  For $17 for a co...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5857</th>\n",
       "      <td>2012-09-27</td>\n",
       "      <td>R14ZGYPSP9H0Y7</td>\n",
       "      <td>Pretzel</td>\n",
       "      <td>A must read</td>\n",
       "      <td>The depth of character development and storyli...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5858</th>\n",
       "      <td>2012-09-27</td>\n",
       "      <td>R1913ISIDAGQ1A</td>\n",
       "      <td>Prodigy</td>\n",
       "      <td>I love it</td>\n",
       "      <td>The book was great and I will love to re-read ...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5859</th>\n",
       "      <td>2012-09-27</td>\n",
       "      <td>R2JY771IW7RI3R</td>\n",
       "      <td>David Katz</td>\n",
       "      <td>Kendle price too expensive</td>\n",
       "      <td>I started to order the kindle edition and than...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5860</th>\n",
       "      <td>2012-09-27</td>\n",
       "      <td>R22B7K1DUJR6ZN</td>\n",
       "      <td>M. A. Barnett</td>\n",
       "      <td>too expensive</td>\n",
       "      <td>I would love to buy this book but it is too ex...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5861 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date              id           author  \\\n",
       "0     2017-04-21  R3TUANQ2EB3ECB  MichaelMichaels   \n",
       "1     2017-04-20  R2DD03ZZ4218VW    Frans van Wyk   \n",
       "2     2017-04-20  R296NVKLH5QS4W      Sabina Duke   \n",
       "3     2017-04-05  R3MP7W8LH6VHU8         Jen Blau   \n",
       "4     2017-04-04   RZWP48RKJCXT1   Lilith Eleanor   \n",
       "...          ...             ...              ...   \n",
       "5856  2012-09-27   RT2TE0W92SL67        Tricia K.   \n",
       "5857  2012-09-27  R14ZGYPSP9H0Y7          Pretzel   \n",
       "5858  2012-09-27  R1913ISIDAGQ1A          Prodigy   \n",
       "5859  2012-09-27  R2JY771IW7RI3R       David Katz   \n",
       "5860  2012-09-27  R22B7K1DUJR6ZN    M. A. Barnett   \n",
       "\n",
       "                                                  title  \\\n",
       "0                           Skip it. Life is too short.   \n",
       "1                                            Four Stars   \n",
       "2                                            Characters   \n",
       "3                                     GIVE IT A CHANCE!   \n",
       "4                                    Frighteningly good   \n",
       "...                                                 ...   \n",
       "5856  Seriously?  $17 bucks for a computer file???  ...   \n",
       "5857                                        A must read   \n",
       "5858                                          I love it   \n",
       "5859                         Kendle price too expensive   \n",
       "5860                                      too expensive   \n",
       "\n",
       "                                                   body  star_rating  \n",
       "0     I've never read any of the Harry Potter books ...          1.0  \n",
       "1     Excellent Read with a lot of real life values ...          4.0  \n",
       "2                  Hard to keep the characters straight          4.0  \n",
       "3     I almost put this book down. I'm new to Rowlin...          5.0  \n",
       "4     Amazing. Rowling combines fantastic writing wi...          5.0  \n",
       "...                                                 ...          ...  \n",
       "5856  Premise sounds dull as dirt.  For $17 for a co...          1.0  \n",
       "5857  The depth of character development and storyli...          5.0  \n",
       "5858  The book was great and I will love to re-read ...          5.0  \n",
       "5859  I started to order the kindle edition and than...          5.0  \n",
       "5860  I would love to buy this book but it is too ex...          1.0  \n",
       "\n",
       "[5861 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.drop(['date', 'id', 'author', 'title'],\n",
    "    axis = 1,\n",
    "    inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>star_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I've never read any of the Harry Potter books ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Excellent Read with a lot of real life values ...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hard to keep the characters straight</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I almost put this book down. I'm new to Rowlin...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amazing. Rowling combines fantastic writing wi...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5856</th>\n",
       "      <td>Premise sounds dull as dirt.  For $17 for a co...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5857</th>\n",
       "      <td>The depth of character development and storyli...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5858</th>\n",
       "      <td>The book was great and I will love to re-read ...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5859</th>\n",
       "      <td>I started to order the kindle edition and than...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5860</th>\n",
       "      <td>I would love to buy this book but it is too ex...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5861 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   body  star_rating\n",
       "0     I've never read any of the Harry Potter books ...          1.0\n",
       "1     Excellent Read with a lot of real life values ...          4.0\n",
       "2                  Hard to keep the characters straight          4.0\n",
       "3     I almost put this book down. I'm new to Rowlin...          5.0\n",
       "4     Amazing. Rowling combines fantastic writing wi...          5.0\n",
       "...                                                 ...          ...\n",
       "5856  Premise sounds dull as dirt.  For $17 for a co...          1.0\n",
       "5857  The depth of character development and storyli...          5.0\n",
       "5858  The book was great and I will love to re-read ...          5.0\n",
       "5859  I started to order the kindle edition and than...          5.0\n",
       "5860  I would love to buy this book but it is too ex...          1.0\n",
       "\n",
       "[5861 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `NaN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positive, neutral, and negatives reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    2711\n",
       "-1    2177\n",
       " 0     970\n",
       "Name: reviews, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['reviews'] = df.star_rating.map({1:-1, 2:-1, 3:0, 4:1, 5:1})\n",
    "df.reviews.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    2711\n",
       "-1    2177\n",
       " 0     970\n",
       "Name: reviews, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ns=df.reviews.value_counts()\n",
    "ns.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for reviews in (-1,0,1):\n",
    "    n = ns[reviews] - ns.min()\n",
    "    index = df[df.reviews == reviews].sample(n=n, random_state = 0).index\n",
    "    df.drop(index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    970\n",
       "-1    970\n",
       " 0    970\n",
       "Name: reviews, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reviews.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature matrix and response vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=df.body\n",
    "c=df.reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_X, test_X, train_c, test_c = model_selection.train_test_split(X, c, stratify = c, train_size = .6, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF and `TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = feature_extraction.text.TfidfVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "class CustomTokenizer(object):\n",
    "    def _call_(self, document):\n",
    "        tokens = tokenize.text(document)\n",
    "        tokens = stemmer.stem_tokens(tokens)\n",
    "        return tokens\n",
    "'''\n",
    "\n",
    "class CustomTokenizer(object):\n",
    "    def __call__(self, document):\n",
    "        tokens = tokenize_text(document)\n",
    "        tokens = Stemmer.stem_tokens(tokens)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "TfidfVectorizer - Vocabulary wasn't fitted.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-1d9550bfa4c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/eunicelau/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents, copy)\u001b[0m\n\u001b[1;32m   1378\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_tfidf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'The tfidf vector is not fitted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1380\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1381\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/eunicelau/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m    888\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m         \u001b[0;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/eunicelau/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m_check_vocabulary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;34m\"\"\"Check if vocabulary is empty or missing (not fit-ed)\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"%(name)s - Vocabulary wasn't fitted.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vocabulary_'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/eunicelau/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0;31m# FIXME NotFittedError_ --> NotFittedError in 0.19\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_NotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: TfidfVectorizer - Vocabulary wasn't fitted."
     ]
    }
   ],
   "source": [
    "train_X = vectorizer.transform(train_X)\n",
    "test_X = vectorizer.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "lower not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-8bfbaa87089b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/eunicelau/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1330\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m         \"\"\"\n\u001b[0;32m-> 1332\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1333\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/eunicelau/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 839\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/eunicelau/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/eunicelau/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 241\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/eunicelau/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/eunicelau/anaconda/lib/python2.7/site-packages/scipy/sparse/base.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: lower not found"
     ]
    }
   ],
   "source": [
    "vectorizer.fit(train_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'00',\n",
       " u'000',\n",
       " u'01',\n",
       " u'08',\n",
       " u'10',\n",
       " u'100',\n",
       " u'1000',\n",
       " u'105',\n",
       " u'11',\n",
       " u'110',\n",
       " u'12',\n",
       " u'120',\n",
       " u'13',\n",
       " u'130',\n",
       " u'132',\n",
       " u'14',\n",
       " u'142',\n",
       " u'143',\n",
       " u'149',\n",
       " u'15',\n",
       " u'150',\n",
       " u'17',\n",
       " u'170',\n",
       " u'175',\n",
       " u'18',\n",
       " u'1860',\n",
       " u'1950',\n",
       " u'1960s',\n",
       " u'1984',\n",
       " u'19th',\n",
       " u'20',\n",
       " u'200',\n",
       " u'2012',\n",
       " u'2015',\n",
       " u'2016',\n",
       " u'21',\n",
       " u'21st',\n",
       " u'23',\n",
       " u'236',\n",
       " u'24',\n",
       " u'25',\n",
       " u'250',\n",
       " u'27',\n",
       " u'28',\n",
       " u'2nd',\n",
       " u'2star',\n",
       " u'30',\n",
       " u'300',\n",
       " u'30am',\n",
       " u'30s',\n",
       " u'31',\n",
       " u'32',\n",
       " u'35',\n",
       " u'380',\n",
       " u'3rd',\n",
       " u'40',\n",
       " u'400',\n",
       " u'44',\n",
       " u'45',\n",
       " u'450',\n",
       " u'50',\n",
       " u'500',\n",
       " u'500th',\n",
       " u'503',\n",
       " u'505',\n",
       " u'50th',\n",
       " u'57',\n",
       " u'60',\n",
       " u'600',\n",
       " u'62',\n",
       " u'6th',\n",
       " u'70',\n",
       " u'72',\n",
       " u'75',\n",
       " u'77',\n",
       " u'80',\n",
       " u'800',\n",
       " u'89',\n",
       " u'8th',\n",
       " u'90',\n",
       " u'92',\n",
       " u'93',\n",
       " u'95',\n",
       " u'97',\n",
       " u'98',\n",
       " u'99',\n",
       " u'_the',\n",
       " u'aand',\n",
       " u'aback',\n",
       " u'abandon',\n",
       " u'abandoned',\n",
       " u'ability',\n",
       " u'abillity',\n",
       " u'abject',\n",
       " u'able',\n",
       " u'abnormal',\n",
       " u'abortion',\n",
       " u'abound',\n",
       " u'aboutthe',\n",
       " u'abraham',\n",
       " u'abrupt',\n",
       " u'abruptly',\n",
       " u'absence',\n",
       " u'absentmindedly',\n",
       " u'absolute',\n",
       " u'absolutely',\n",
       " u'absorb',\n",
       " u'absorbed',\n",
       " u'absorbing',\n",
       " u'absorption',\n",
       " u'absurdity',\n",
       " u'abundance',\n",
       " u'abundant',\n",
       " u'abuse',\n",
       " u'abused',\n",
       " u'abusing',\n",
       " u'abusive',\n",
       " u'abutting',\n",
       " u'accent',\n",
       " u'accents',\n",
       " u'accept',\n",
       " u'acceptable',\n",
       " u'accepted',\n",
       " u'accesses',\n",
       " u'accident',\n",
       " u'accidental',\n",
       " u'accomplish',\n",
       " u'accomplished',\n",
       " u'accomplishes',\n",
       " u'according',\n",
       " u'account',\n",
       " u'accounts',\n",
       " u'accuracy',\n",
       " u'accurate',\n",
       " u'accused',\n",
       " u'accustomed',\n",
       " u'acerbic',\n",
       " u'ache',\n",
       " u'achieve',\n",
       " u'achievement',\n",
       " u'achievment',\n",
       " u'acknowledge',\n",
       " u'acorn',\n",
       " u'acquaintances',\n",
       " u'act',\n",
       " u'acting',\n",
       " u'action',\n",
       " u'actions',\n",
       " u'actively',\n",
       " u'activites',\n",
       " u'activities',\n",
       " u'actor',\n",
       " u'actress',\n",
       " u'acts',\n",
       " u'actual',\n",
       " u'actually',\n",
       " u'acumen',\n",
       " u'acute',\n",
       " u'acutely',\n",
       " u'ad',\n",
       " u'adapt',\n",
       " u'adaptation',\n",
       " u'adaptations',\n",
       " u'add',\n",
       " u'added',\n",
       " u'addict',\n",
       " u'addicted',\n",
       " u'addiction',\n",
       " u'adding',\n",
       " u'addition',\n",
       " u'additional',\n",
       " u'address',\n",
       " u'addressed',\n",
       " u'adds',\n",
       " u'adept',\n",
       " u'adeptly',\n",
       " u'adequate',\n",
       " u'adjacent',\n",
       " u'adjectives',\n",
       " u'adjust',\n",
       " u'adjusted',\n",
       " u'adloescents',\n",
       " u'administered',\n",
       " u'admirable',\n",
       " u'admiration',\n",
       " u'admire',\n",
       " u'admired',\n",
       " u'admit',\n",
       " u'admittedly',\n",
       " u'adolescence',\n",
       " u'adolescent',\n",
       " u'adolescents',\n",
       " u'adore',\n",
       " u'adored',\n",
       " u'adrian',\n",
       " u'adult',\n",
       " u'adulthood',\n",
       " u'adults',\n",
       " u'aduts',\n",
       " u'advantage',\n",
       " u'adventure',\n",
       " u'adventures',\n",
       " u'adventurous',\n",
       " u'advertising',\n",
       " u'advertizing',\n",
       " u'advice',\n",
       " u'advise',\n",
       " u'advocate',\n",
       " u'affair',\n",
       " u'affect',\n",
       " u'affected',\n",
       " u'affection',\n",
       " u'affects',\n",
       " u'affluent',\n",
       " u'afraid',\n",
       " u'aftermath',\n",
       " u'afternoon',\n",
       " u'afterward',\n",
       " u'agatha',\n",
       " u'age',\n",
       " u'aged',\n",
       " u'ageless',\n",
       " u'agendas',\n",
       " u'agent',\n",
       " u'ages',\n",
       " u'aggressive',\n",
       " u'agitating',\n",
       " u'ago',\n",
       " u'agree',\n",
       " u'agreed',\n",
       " u'ahead',\n",
       " u'aid',\n",
       " u'aiming',\n",
       " u'aimless',\n",
       " u'aims',\n",
       " u'ain',\n",
       " u'air',\n",
       " u'airing',\n",
       " u'airport',\n",
       " u'aka',\n",
       " u'akin',\n",
       " u'al',\n",
       " u'ala',\n",
       " u'alas',\n",
       " u'albeit',\n",
       " u'alcohol',\n",
       " u'alcoholism',\n",
       " u'alert',\n",
       " u'alexander',\n",
       " u'alike',\n",
       " u'alive',\n",
       " u'alleged',\n",
       " u'allegiance',\n",
       " u'allegience',\n",
       " u'allegorical',\n",
       " u'alley',\n",
       " u'allot',\n",
       " u'allow',\n",
       " u'allowed',\n",
       " u'allowing',\n",
       " u'allows',\n",
       " u'alls',\n",
       " u'allusion',\n",
       " u'allvery',\n",
       " u'ally',\n",
       " u'alot',\n",
       " u'alright',\n",
       " u'alternate',\n",
       " u'alternately',\n",
       " u'alternates',\n",
       " u'altogether',\n",
       " u'altruism',\n",
       " u'amateurish',\n",
       " u'amazed',\n",
       " u'amazes',\n",
       " u'amazing',\n",
       " u'amazingly',\n",
       " u'amazom',\n",
       " u'amazon',\n",
       " u'amazonon',\n",
       " u'ambiguous',\n",
       " u'ambition',\n",
       " u'ambitious',\n",
       " u'ambivalent',\n",
       " u'america',\n",
       " u'american',\n",
       " u'americans',\n",
       " u'amidst',\n",
       " u'amiss',\n",
       " u'ample',\n",
       " u'amusing',\n",
       " u'amy',\n",
       " u'analogies',\n",
       " u'andy',\n",
       " u'anecdotes',\n",
       " u'aneurysm',\n",
       " u'angels',\n",
       " u'anger',\n",
       " u'angle',\n",
       " u'angles',\n",
       " u'anglophile',\n",
       " u'anglophiles',\n",
       " u'angry',\n",
       " u'angst',\n",
       " u'ann',\n",
       " u'anniversary',\n",
       " u'announced',\n",
       " u'announcement',\n",
       " u'annoying',\n",
       " u'answered',\n",
       " u'antagonist',\n",
       " u'antagonists',\n",
       " u'anti',\n",
       " u'anticipate',\n",
       " u'anticipated',\n",
       " u'anticipating',\n",
       " u'anticipation',\n",
       " u'anticlimactic',\n",
       " u'antiheroes',\n",
       " u'antipating',\n",
       " u'antithesis',\n",
       " u'anxiety',\n",
       " u'anxious',\n",
       " u'anybody',\n",
       " u'anyday',\n",
       " u'anymore',\n",
       " u'anytown',\n",
       " u'apart',\n",
       " u'apartments',\n",
       " u'apathy',\n",
       " u'apologetic',\n",
       " u'appalled',\n",
       " u'appalling',\n",
       " u'apparent',\n",
       " u'apparently',\n",
       " u'appeal',\n",
       " u'appealing',\n",
       " u'appeals',\n",
       " u'appear',\n",
       " u'appearance',\n",
       " u'appearances',\n",
       " u'appeared',\n",
       " u'appears',\n",
       " u'applaud',\n",
       " u'applause',\n",
       " u'applied',\n",
       " u'applies',\n",
       " u'appreciate',\n",
       " u'appreciated',\n",
       " u'appreciates',\n",
       " u'apprehensive',\n",
       " u'apprentice',\n",
       " u'approach',\n",
       " u'approaches',\n",
       " u'appropriate',\n",
       " u'appropriately',\n",
       " u'approve',\n",
       " u'approximately',\n",
       " u'apt',\n",
       " u'arc',\n",
       " u'arch',\n",
       " u'archetypal',\n",
       " u'arcs',\n",
       " u'area',\n",
       " u'areas',\n",
       " u'aren',\n",
       " u'arena',\n",
       " u'arf',\n",
       " u'arguably',\n",
       " u'argument',\n",
       " u'arguments',\n",
       " u'arise',\n",
       " u'arises',\n",
       " u'armies',\n",
       " u'arms',\n",
       " u'arose',\n",
       " u'arouse',\n",
       " u'arranging',\n",
       " u'array',\n",
       " u'arrival',\n",
       " u'arrived',\n",
       " u'arrogance',\n",
       " u'arrogant',\n",
       " u'art',\n",
       " u'artfully',\n",
       " u'artistically',\n",
       " u'artistry',\n",
       " u'asan',\n",
       " u'asap',\n",
       " u'ashamed',\n",
       " u'aside',\n",
       " u'ask',\n",
       " u'asked',\n",
       " u'asking',\n",
       " u'asks',\n",
       " u'asleep',\n",
       " u'aspect',\n",
       " u'aspects',\n",
       " u'aspire',\n",
       " u'assault',\n",
       " u'assaulted',\n",
       " u'assessing',\n",
       " u'assessment',\n",
       " u'asside',\n",
       " u'assign',\n",
       " u'assigned',\n",
       " u'associate',\n",
       " u'associations',\n",
       " u'assuage',\n",
       " u'assume',\n",
       " u'assumed',\n",
       " u'assuming',\n",
       " u'assumptions',\n",
       " u'assured',\n",
       " u'astonished',\n",
       " u'astonishingly',\n",
       " u'astounded',\n",
       " u'astounding',\n",
       " u'astoundingly',\n",
       " u'astute',\n",
       " u'ate',\n",
       " u'athlete',\n",
       " u'atmosphere',\n",
       " u'attach',\n",
       " u'attached',\n",
       " u'attachment',\n",
       " u'attack',\n",
       " u'attempt',\n",
       " u'attempting',\n",
       " u'attempts',\n",
       " u'attended',\n",
       " u'attention',\n",
       " u'attitude',\n",
       " u'attitudes',\n",
       " u'attorney',\n",
       " u'attracted',\n",
       " u'audible',\n",
       " u'audience',\n",
       " u'audiences',\n",
       " u'audio',\n",
       " u'audiobook',\n",
       " u'audiobooks',\n",
       " u'aunt',\n",
       " u'austen',\n",
       " u'australia',\n",
       " u'authentic',\n",
       " u'authenticity',\n",
       " u'author',\n",
       " u'authored',\n",
       " u'authoress',\n",
       " u'authorial',\n",
       " u'authors',\n",
       " u'available',\n",
       " u'avalanche',\n",
       " u'average',\n",
       " u'avid',\n",
       " u'avoid',\n",
       " u'avoided',\n",
       " u'avoiding',\n",
       " u'awaiting',\n",
       " u'awake',\n",
       " u'aware',\n",
       " u'awareness',\n",
       " u'away',\n",
       " u'awe',\n",
       " u'awed',\n",
       " u'awesome',\n",
       " u'awful',\n",
       " u'awhile',\n",
       " u'awkward',\n",
       " u'awkwardly',\n",
       " u'baby',\n",
       " u'backbiting',\n",
       " u'backdoor',\n",
       " u'backdrop',\n",
       " u'backfired',\n",
       " u'background',\n",
       " u'backgrounds',\n",
       " u'backing',\n",
       " u'backstabbing',\n",
       " u'backstory',\n",
       " u'backwards',\n",
       " u'bad',\n",
       " u'badly',\n",
       " u'bag',\n",
       " u'bagged',\n",
       " u'balance',\n",
       " u'balanced',\n",
       " u'ball',\n",
       " u'balls',\n",
       " u'banal',\n",
       " u'banality',\n",
       " u'band',\n",
       " u'barbara',\n",
       " u'bare',\n",
       " u'barely',\n",
       " u'bares',\n",
       " u'barry',\n",
       " u'barrymore',\n",
       " u'base',\n",
       " u'baseball',\n",
       " u'based',\n",
       " u'bashing',\n",
       " u'basic',\n",
       " u'basically',\n",
       " u'basing',\n",
       " u'basis',\n",
       " u'basket',\n",
       " u'basketball',\n",
       " u'bat',\n",
       " u'bated',\n",
       " u'bath',\n",
       " u'bathroom',\n",
       " u'bathrooms',\n",
       " u'bbc',\n",
       " u'bc',\n",
       " u'beach',\n",
       " u'bear',\n",
       " u'bearing',\n",
       " u'beasts',\n",
       " u'beat',\n",
       " u'beaters',\n",
       " u'beats',\n",
       " u'beautiful',\n",
       " u'beautifully',\n",
       " u'beauty',\n",
       " u'becacause',\n",
       " u'bed',\n",
       " u'bedroom',\n",
       " u'bedtime',\n",
       " u'began',\n",
       " u'beget',\n",
       " u'begin',\n",
       " u'beginning',\n",
       " u'begins',\n",
       " u'beguiled',\n",
       " u'behave',\n",
       " u'behaved',\n",
       " u'behaving',\n",
       " u'behavior',\n",
       " u'behaviors',\n",
       " u'behaviour',\n",
       " u'behaviours',\n",
       " u'beingness',\n",
       " u'beings',\n",
       " u'belief',\n",
       " u'beliefs',\n",
       " u'believable',\n",
       " u'believe',\n",
       " u'believeable',\n",
       " u'believed',\n",
       " u'believes',\n",
       " u'believing',\n",
       " u'bellchapel',\n",
       " u'belligerant',\n",
       " u'belly',\n",
       " u'belong',\n",
       " u'belonged',\n",
       " u'belongs',\n",
       " u'beloved',\n",
       " u'beneath',\n",
       " u'benefit',\n",
       " u'benefited',\n",
       " u'beset',\n",
       " u'best',\n",
       " u'bestseller',\n",
       " u'betrayal',\n",
       " u'betrayed',\n",
       " u'better',\n",
       " u'bevy',\n",
       " u'biased',\n",
       " u'bible',\n",
       " u'bibliophile',\n",
       " u'bickering',\n",
       " u'big',\n",
       " u'bigger',\n",
       " u'biggest',\n",
       " u'bigotry',\n",
       " u'billion',\n",
       " u'billionaire',\n",
       " u'binchy',\n",
       " u'bind',\n",
       " u'binding',\n",
       " u'bing',\n",
       " u'bird',\n",
       " u'birthday',\n",
       " u'bit',\n",
       " u'biting',\n",
       " u'bits',\n",
       " u'bitter',\n",
       " u'bitterlky',\n",
       " u'bitterness',\n",
       " u'black',\n",
       " u'blades',\n",
       " u'blame',\n",
       " u'blanch',\n",
       " u'bland',\n",
       " u'blank',\n",
       " u'blatantly',\n",
       " u'bleak',\n",
       " u'bleaker',\n",
       " u'bleakness',\n",
       " u'bleeding',\n",
       " u'bleeds',\n",
       " u'blend',\n",
       " u'blessed',\n",
       " u'blessing',\n",
       " u'blind',\n",
       " u'blistering',\n",
       " u'bloated',\n",
       " u'blockbuster',\n",
       " u'blog',\n",
       " u'blood',\n",
       " u'bloodbath',\n",
       " u'bloody',\n",
       " u'blossom',\n",
       " u'blow',\n",
       " u'blows',\n",
       " u'blue',\n",
       " u'blunt',\n",
       " u'blurb',\n",
       " u'blurring',\n",
       " u'board',\n",
       " u'boared',\n",
       " u'bogged',\n",
       " u'boggs',\n",
       " u'boldly',\n",
       " u'boldness',\n",
       " u'bomb',\n",
       " u'bombs',\n",
       " u'bone',\n",
       " u'bones',\n",
       " u'boo',\n",
       " u'book',\n",
       " u'booker',\n",
       " u'bookmarked',\n",
       " u'books',\n",
       " u'bookso',\n",
       " u'bookstore',\n",
       " u'booming',\n",
       " u'boorish',\n",
       " u'border',\n",
       " u'bordered',\n",
       " u'bordering',\n",
       " u'borders',\n",
       " u'bore',\n",
       " u'bored',\n",
       " u'boredom',\n",
       " u'boreing',\n",
       " u'boring',\n",
       " u'borrow',\n",
       " u'borrowing',\n",
       " u'bother',\n",
       " u'bothered',\n",
       " u'bought',\n",
       " u'bounces',\n",
       " u'bow',\n",
       " u'box',\n",
       " u'boy',\n",
       " u'boyfriend',\n",
       " u'boys',\n",
       " u'brace',\n",
       " u'bracing',\n",
       " u'brain',\n",
       " u'branch',\n",
       " u'brand',\n",
       " u'brava',\n",
       " u'bravado',\n",
       " u'brave',\n",
       " u'bravo',\n",
       " u'breadth',\n",
       " u'break',\n",
       " u'breakaway',\n",
       " u'breaker',\n",
       " u'breaking',\n",
       " u'breaks',\n",
       " u'breasts',\n",
       " u'breath',\n",
       " u'breathe',\n",
       " u'breathtaking',\n",
       " u'breed',\n",
       " u'bridgewater',\n",
       " u'brief',\n",
       " u'briefly',\n",
       " u'brighter',\n",
       " u'brightest',\n",
       " u'brillant',\n",
       " u'brilliance',\n",
       " u'brilliant',\n",
       " u'brilliantly',\n",
       " u'bring',\n",
       " u'bringing',\n",
       " u'brings',\n",
       " u'brit',\n",
       " u'britain',\n",
       " u'british',\n",
       " u'britisk',\n",
       " u'britney',\n",
       " u'brits',\n",
       " u'broad',\n",
       " u'broken',\n",
       " u'brookside',\n",
       " u'broomsticks',\n",
       " u'brought',\n",
       " u'brown',\n",
       " u'brutal',\n",
       " u'brutally',\n",
       " u'buckets',\n",
       " u'bucks',\n",
       " u'bucolic',\n",
       " u'budget',\n",
       " u'buffet',\n",
       " u'buffoons',\n",
       " u'build',\n",
       " u'building',\n",
       " u'builds',\n",
       " u'built',\n",
       " u'bullies',\n",
       " u'bulling',\n",
       " u'bully',\n",
       " u'bullying',\n",
       " u'bummed',\n",
       " u'bummer',\n",
       " u'bunch',\n",
       " u'burner',\n",
       " u'burst',\n",
       " u'bus',\n",
       " u'business',\n",
       " u'busy',\n",
       " u'button',\n",
       " u'buy',\n",
       " u'buying',\n",
       " u'calamities',\n",
       " u'calendar',\n",
       " u'caliber',\n",
       " u'called',\n",
       " u'calling',\n",
       " u'came',\n",
       " u'camp',\n",
       " u'campaigns',\n",
       " u'camps',\n",
       " u'canary',\n",
       " u'candidate',\n",
       " u'candidates',\n",
       " u'candle',\n",
       " u'canon',\n",
       " u'capability',\n",
       " u'capable',\n",
       " u'capacity',\n",
       " u'capitalist',\n",
       " u'capitalize',\n",
       " u'capitilized',\n",
       " u'captivated',\n",
       " u'captivating',\n",
       " u'captive',\n",
       " u'capture',\n",
       " u'captured',\n",
       " u'captures',\n",
       " u'capturing',\n",
       " u'car',\n",
       " u'caracters',\n",
       " u'cardboard',\n",
       " u'care',\n",
       " u'cared',\n",
       " u'career',\n",
       " u'careers',\n",
       " u'careful',\n",
       " u'carefully',\n",
       " u'cares',\n",
       " u'caricature',\n",
       " u'caricatures',\n",
       " u'caring',\n",
       " u'carol',\n",
       " u'carpentry',\n",
       " u'carried',\n",
       " u'carries',\n",
       " u'carry',\n",
       " u'cartoonish',\n",
       " u'case',\n",
       " u'cashed',\n",
       " u'cashing',\n",
       " u'cast',\n",
       " u'caste',\n",
       " u'castle',\n",
       " u'casts',\n",
       " u'casual',\n",
       " u'casually',\n",
       " u'cat',\n",
       " u'cataclysm',\n",
       " u'catalyst',\n",
       " u'catapulted',\n",
       " u'catastrophe',\n",
       " u'catch',\n",
       " u'catcher',\n",
       " u'categorically',\n",
       " u'category',\n",
       " u'caught',\n",
       " u'cauldron',\n",
       " u'causal',\n",
       " u'cause',\n",
       " u'caused',\n",
       " u'causes',\n",
       " u'causing',\n",
       " u'caustic',\n",
       " u'cave',\n",
       " u'caveat',\n",
       " u'cd',\n",
       " u'celebrate',\n",
       " u'celebrity',\n",
       " u'cent',\n",
       " u'center',\n",
       " u'centered',\n",
       " u'centering',\n",
       " u'central',\n",
       " u'centre',\n",
       " u'centred',\n",
       " u'century',\n",
       " u'certain',\n",
       " u'certainly',\n",
       " u'cesspool',\n",
       " u'chadha',\n",
       " u'chain',\n",
       " u'chairman',\n",
       " u'chalked',\n",
       " u'challenge',\n",
       " u'challenged',\n",
       " u'challenges',\n",
       " u'challenging',\n",
       " u'chance',\n",
       " u'chances',\n",
       " u'change',\n",
       " u'changed',\n",
       " u'changes',\n",
       " u'changing',\n",
       " u'chaos',\n",
       " u'chapter',\n",
       " u'chapters',\n",
       " u'character',\n",
       " u'characterisation',\n",
       " u'characterisations',\n",
       " u'characteristically',\n",
       " u'characteristics',\n",
       " u'characterization',\n",
       " u'characterizations',\n",
       " u'characters',\n",
       " u'charactor',\n",
       " u'charactors',\n",
       " u'charcaters',\n",
       " u'charcter',\n",
       " u'charge',\n",
       " u'charitable',\n",
       " u'charity',\n",
       " u'charlie',\n",
       " u'charm',\n",
       " u'charmed',\n",
       " u'charming',\n",
       " u'charms',\n",
       " u'chart',\n",
       " u'chase',\n",
       " u'chatter',\n",
       " u'chatty',\n",
       " u'cheap',\n",
       " u'cheat',\n",
       " u'cheating',\n",
       " u'check',\n",
       " u'checked',\n",
       " u'checking',\n",
       " u'checklist',\n",
       " u'cheer',\n",
       " u'cheered',\n",
       " u'cheerful',\n",
       " u'cheerfulness',\n",
       " u'cheers',\n",
       " u'chewing',\n",
       " u'chic',\n",
       " u'chief',\n",
       " u'child',\n",
       " u'childhood',\n",
       " u'children',\n",
       " u'childrens',\n",
       " u'chill',\n",
       " u'chimera',\n",
       " u'chip',\n",
       " u'chock',\n",
       " u'choice',\n",
       " u'choices',\n",
       " u'choose',\n",
       " u'chooses',\n",
       " u'chop',\n",
       " u'chopped',\n",
       " u'choppy',\n",
       " u'choral',\n",
       " u'chore',\n",
       " u'chose',\n",
       " u'chosen',\n",
       " u'chosing',\n",
       " u'christie',\n",
       " u'christmas',\n",
       " u'chronicling',\n",
       " u'chuckles',\n",
       " u'churn',\n",
       " u'cinematically',\n",
       " u'circle',\n",
       " u'circling',\n",
       " u'circumstance',\n",
       " u'circumstances',\n",
       " u'citizen',\n",
       " u'citizens',\n",
       " u'city',\n",
       " u'civil',\n",
       " u'claimed',\n",
       " u'clan',\n",
       " u'claptrap',\n",
       " u'clarence',\n",
       " u'clarity',\n",
       " u'class',\n",
       " u'classes',\n",
       " u'classic',\n",
       " u'classical',\n",
       " u'classify',\n",
       " u'classism',\n",
       " u'classmates',\n",
       " u'classroom',\n",
       " u'classy',\n",
       " u'claustrophobic',\n",
       " u'clean',\n",
       " u'cleaned',\n",
       " u'clear',\n",
       " u'clearer',\n",
       " u'clearly',\n",
       " u'clever',\n",
       " u'cliche',\n",
       " u'cliched',\n",
       " u'cliches',\n",
       " u'click',\n",
       " u'cliff',\n",
       " u'cliffhangers',\n",
       " u'climactic',\n",
       " u'climatic',\n",
       " u'climax',\n",
       " u'climb',\n",
       " u'cling',\n",
       " u'clinic',\n",
       " u'clinical',\n",
       " u'clinics',\n",
       " u'clip',\n",
       " u'clips',\n",
       " u'close',\n",
       " u'closed',\n",
       " u'closely',\n",
       " u'closer',\n",
       " u'closest',\n",
       " u'closet',\n",
       " u'closing',\n",
       " u'closure',\n",
       " u'cloud',\n",
       " u'cloudy',\n",
       " u'cloying',\n",
       " u'club',\n",
       " u'clue',\n",
       " u'clueless',\n",
       " u'clumsy',\n",
       " u'clusters',\n",
       " u'cluttered',\n",
       " u'coarse',\n",
       " u'coaster',\n",
       " u'cobbled',\n",
       " u'coded',\n",
       " u'codes',\n",
       " u'cohesive',\n",
       " u'cold',\n",
       " u'collapses',\n",
       " u'colleagues',\n",
       " u'collection',\n",
       " u'college',\n",
       " u'collision',\n",
       " u'collusion',\n",
       " u'color',\n",
       " u'colored',\n",
       " u'colorful',\n",
       " u'colorless',\n",
       " u'colors',\n",
       " u'columns',\n",
       " u'com',\n",
       " u'combated',\n",
       " u'combined',\n",
       " u'come',\n",
       " u'comedy',\n",
       " u'comes',\n",
       " u'comeuppance',\n",
       " u'comfort',\n",
       " u'comfortable',\n",
       " u'comfortably',\n",
       " u'comforting',\n",
       " u'comic',\n",
       " u'comical',\n",
       " u'coming',\n",
       " u'command',\n",
       " u'commandant',\n",
       " u'commands',\n",
       " ...]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformed feature matrix `X`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # TODO..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
